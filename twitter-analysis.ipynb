{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter analysis \n",
    "\n",
    "<center><img src=\"https://techcrunch.com/wp-content/uploads/2017/12/gettyimages-876768474.jpg?w=730&crop=1\"></center>\n",
    "\n",
    "Analyze the sentiment of tweets on some query search, your model should learn from [this dataset](https://www.kaggle.com/kazanova/sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0          scotthamilton   \n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../dataset/training.1600000.processed.noemoticon.csv',encoding='latin',sep=',')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns=['target','ids','date','flag','user','text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          is upset that he can't update his Facebook by ...\n",
       "1          @Kenichan I dived many times for the ball. Man...\n",
       "2            my whole body feels itchy and like its on fire \n",
       "3          @nationwideclass no, it's not behaving at all....\n",
       "4                              @Kwesidei not the whole crew \n",
       "                                 ...                        \n",
       "1599994    Just woke up. Having no school is the best fee...\n",
       "1599995    TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599996    Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599997    Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599998    happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "Name: text, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset['text']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = dataset['target']\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    800000\n",
       "negative    799999\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment(polarity):\n",
    "    if polarity == 3 or polarity == 4 or polarity == 2:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "target = target.apply(sentiment)\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def prepare_text(text):\n",
    "    \n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'[\\W\\d_]',' ',text)\n",
    "    words = text.split()\n",
    "    words = [lemm.lemmatize(word,pos='v') for word in words]\n",
    "    joined = ' '.join(words)\n",
    "    \n",
    "    return joined.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          be upset that he can t update his facebook by ...\n",
       "1          kenichan i dive many time for the ball manage ...\n",
       "2              my whole body feel itchy and like its on fire\n",
       "3          nationwideclass no it s not behave at all i m ...\n",
       "4                                kwesidei not the whole crew\n",
       "                                 ...                        \n",
       "1599994    just wake up have no school be the best feel ever\n",
       "1599995    thewdb com very cool to hear old walt intervie...\n",
       "1599996    be you ready for your mojo makeover ask me for...\n",
       "1599997    happy th birthday to my boo of alll time tupac...\n",
       "1599998    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,target,test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english',max_features=100000)\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.76      0.76      8090\n",
      "    positive       0.75      0.75      0.75      7910\n",
      "\n",
      "    accuracy                           0.75     16000\n",
      "   macro avg       0.75      0.75      0.75     16000\n",
      "weighted avg       0.75      0.75      0.75     16000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6110</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947</td>\n",
       "      <td>5963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  6110  1980\n",
       "1  1947  5963"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.74      0.76      8090\n",
      "    positive       0.75      0.79      0.77      7910\n",
      "\n",
      "    accuracy                           0.77     16000\n",
      "   macro avg       0.77      0.77      0.77     16000\n",
      "weighted avg       0.77      0.77      0.77     16000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6023</td>\n",
       "      <td>2067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1691</td>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  6023  2067\n",
       "1  1691  6219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf3 = LinearSVC()\n",
    "clf3.fit(x_train,y_train)\n",
    "y_predicted = clf3.predict(x_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.68      0.73      8090\n",
      "    positive       0.71      0.82      0.76      7910\n",
      "\n",
      "    accuracy                           0.75     16000\n",
      "   macro avg       0.75      0.75      0.75     16000\n",
      "weighted avg       0.76      0.75      0.75     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(class_weight='balanced')\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71      7920\n",
      "           4       0.71      0.82      0.76      8080\n",
      "\n",
      "    accuracy                           0.74     16000\n",
      "   macro avg       0.74      0.74      0.74     16000\n",
      "weighted avg       0.74      0.74      0.74     16000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5188</td>\n",
       "      <td>2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>6618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  5188  2732\n",
       "1  1462  6618"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train)\n",
    "y_predicted = xgb.predict(x_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's obvoious that LinearSVC is the best one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's scrap Twetter and classifiy scrapped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to search for ? : Egypt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Egypt since:2006-03-21 until:2006-12-04', 'Egypt since:2006-12-04 until:2007-08-19', 'Egypt since:2007-08-19 until:2008-05-03', 'Egypt since:2008-05-03 until:2009-01-17', 'Egypt since:2009-01-17 until:2009-10-02', 'Egypt since:2009-10-02 until:2010-06-17', 'Egypt since:2010-06-17 until:2011-03-02', 'Egypt since:2011-03-02 until:2011-11-16', 'Egypt since:2011-11-16 until:2012-07-31', 'Egypt since:2012-07-31 until:2013-04-15', 'Egypt since:2013-04-15 until:2013-12-29', 'Egypt since:2013-12-29 until:2014-09-14', 'Egypt since:2014-09-14 until:2015-05-30', 'Egypt since:2015-05-30 until:2016-02-12', 'Egypt since:2016-02-12 until:2016-10-27', 'Egypt since:2016-10-27 until:2017-07-13', 'Egypt since:2017-07-13 until:2018-03-28', 'Egypt since:2018-03-28 until:2018-12-11', 'Egypt since:2018-12-11 until:2019-08-26', 'Egypt since:2019-08-26 until:2020-05-11']\n",
      "INFO: Querying Egypt since:2007-08-19 until:2008-05-03\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2009-01-17%20until%3A2009-10-02&l=\n",
      "INFO: Querying Egypt since:2008-05-03 until:2009-01-17\n",
      "INFO: Querying Egypt since:2009-10-02 until:2010-06-17\n",
      "INFO: Querying Egypt since:2009-01-17 until:2009-10-02\n",
      "INFO: Querying Egypt since:2006-12-04 until:2007-08-19\n",
      "INFO: Querying Egypt since:2013-12-29 until:2014-09-14\n",
      "INFO: Querying Egypt since:2006-03-21 until:2006-12-04\n",
      "INFO: Querying Egypt since:2010-06-17 until:2011-03-02\n",
      "INFO: Querying Egypt since:2013-04-15 until:2013-12-29\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2006-12-04%20until%3A2007-08-19&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2010-06-17%20until%3A2011-03-02&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2008-05-03%20until%3A2009-01-17&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2013-12-29%20until%3A2014-09-14&l=\n",
      "INFO: Querying Egypt since:2011-11-16 until:2012-07-31\n",
      "INFO: Querying Egypt since:2012-07-31 until:2013-04-15\n",
      "INFO: Querying Egypt since:2011-03-02 until:2011-11-16\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2009-10-02%20until%3A2010-06-17&l=\n",
      "INFO: Querying Egypt since:2019-08-26 until:2020-05-11\n",
      "INFO: Querying Egypt since:2018-12-11 until:2019-08-26\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2007-08-19%20until%3A2008-05-03&l=\n",
      "INFO: Querying Egypt since:2018-03-28 until:2018-12-11\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2013-04-15%20until%3A2013-12-29&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2011-03-02%20until%3A2011-11-16&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2012-07-31%20until%3A2013-04-15&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2018-03-28%20until%3A2018-12-11&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2019-08-26%20until%3A2020-05-11&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Querying Egypt since:2017-07-13 until:2018-03-28\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2018-12-11%20until%3A2019-08-26&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2017-07-13%20until%3A2018-03-28&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2011-11-16%20until%3A2012-07-31&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Querying Egypt since:2014-09-14 until:2015-05-30\n",
      "INFO: Querying Egypt since:2016-02-12 until:2016-10-27\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2014-09-14%20until%3A2015-05-30&l=\n",
      "INFO: Querying Egypt since:2016-10-27 until:2017-07-13\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2016-02-12%20until%3A2016-10-27&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2016-10-27%20until%3A2017-07-13&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Querying Egypt since:2015-05-30 until:2016-02-12\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Egypt%20since%3A2015-05-30%20until%3A2016-02-12&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Got 2 tweets for Egypt%20since%3A2006-03-21%20until%3A2006-12-04.\n",
      "INFO: Got 2 tweets (2 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2006-12-04%20until%3A2007-08-19.\n",
      "INFO: Got 22 tweets (20 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2016-10-27%20until%3A2017-07-13.\n",
      "INFO: Got 41 tweets (19 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2013-12-29%20until%3A2014-09-14.\n",
      "INFO: Got 61 tweets (20 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2009-01-17%20until%3A2009-10-02.\n",
      "INFO: Got 80 tweets (19 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2008-05-03%20until%3A2009-01-17.\n",
      "INFO: Got 100 tweets (20 new).\n",
      "INFO: Got 18 tweets for Egypt%20since%3A2009-10-02%20until%3A2010-06-17.\n",
      "INFO: Got 118 tweets (18 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2013-04-15%20until%3A2013-12-29.\n",
      "INFO: Got 138 tweets (20 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2010-06-17%20until%3A2011-03-02.\n",
      "INFO: Got 158 tweets (20 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2015-05-30%20until%3A2016-02-12.\n",
      "INFO: Got 178 tweets (20 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2018-12-11%20until%3A2019-08-26.\n",
      "INFO: Got 197 tweets (19 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2014-09-14%20until%3A2015-05-30.\n",
      "INFO: Got 216 tweets (19 new).\n",
      "INFO: Got 18 tweets for Egypt%20since%3A2017-07-13%20until%3A2018-03-28.\n",
      "INFO: Got 234 tweets (18 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2019-08-26%20until%3A2020-05-11.\n",
      "INFO: Got 253 tweets (19 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2016-02-12%20until%3A2016-10-27.\n",
      "INFO: Got 273 tweets (20 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2012-07-31%20until%3A2013-04-15.\n",
      "INFO: Got 293 tweets (20 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2007-08-19%20until%3A2008-05-03.\n",
      "INFO: Got 312 tweets (19 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2018-03-28%20until%3A2018-12-11.\n",
      "INFO: Got 331 tweets (19 new).\n",
      "INFO: Got 19 tweets for Egypt%20since%3A2011-03-02%20until%3A2011-11-16.\n",
      "INFO: Got 350 tweets (19 new).\n",
      "INFO: Got 20 tweets for Egypt%20since%3A2011-11-16%20until%3A2012-07-31.\n",
      "INFO: Got 370 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved..\n"
     ]
    }
   ],
   "source": [
    "query = input('What do you want to search for ? : ')\n",
    "#list_of_tweets = query_tweets(query, 1)\n",
    "\n",
    "# #print the retrieved tweets to the screen:\n",
    "# for tweet in query_tweets(\"Trump OR Clinton\", 10):\n",
    "#     print(tweet)\n",
    "\n",
    "#Or save the retrieved tweets to file:\n",
    "file = open('output.txt','w+')\n",
    "for tweet in query_tweets(query, 1):\n",
    "    file.write(str(tweet))\n",
    "file.close()\n",
    "print('Saved..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Cairo since:2006-03-21 until:2006-12-04', 'Cairo since:2006-12-04 until:2007-08-19', 'Cairo since:2007-08-19 until:2008-05-03', 'Cairo since:2008-05-03 until:2009-01-17', 'Cairo since:2009-01-17 until:2009-10-02', 'Cairo since:2009-10-02 until:2010-06-17', 'Cairo since:2010-06-17 until:2011-03-02', 'Cairo since:2011-03-02 until:2011-11-16', 'Cairo since:2011-11-16 until:2012-07-31', 'Cairo since:2012-07-31 until:2013-04-15', 'Cairo since:2013-04-15 until:2013-12-29', 'Cairo since:2013-12-29 until:2014-09-14', 'Cairo since:2014-09-14 until:2015-05-30', 'Cairo since:2015-05-30 until:2016-02-12', 'Cairo since:2016-02-12 until:2016-10-27', 'Cairo since:2016-10-27 until:2017-07-13', 'Cairo since:2017-07-13 until:2018-03-28', 'Cairo since:2018-03-28 until:2018-12-11', 'Cairo since:2018-12-11 until:2019-08-26', 'Cairo since:2019-08-26 until:2020-05-11']\n",
      "INFO: Querying Cairo since:2006-12-04 until:2007-08-19\n",
      "INFO: Querying Cairo since:2009-01-17 until:2009-10-02\n",
      "INFO: Querying Cairo since:2007-08-19 until:2008-05-03\n",
      "INFO: Querying Cairo since:2011-11-16 until:2012-07-31\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-12-04%20until%3A2007-08-19&l=\n",
      "INFO: Querying Cairo since:2008-05-03 until:2009-01-17\n",
      "INFO: Querying Cairo since:2016-02-12 until:2016-10-27\n",
      "INFO: Querying Cairo since:2013-12-29 until:2014-09-14\n",
      "INFO: Querying Cairo since:2011-03-02 until:2011-11-16\n",
      "INFO: Querying Cairo since:2006-03-21 until:2006-12-04\n",
      "INFO: Querying Cairo since:2014-09-14 until:2015-05-30\n",
      "INFO: Querying Cairo since:2016-10-27 until:2017-07-13\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2007-08-19%20until%3A2008-05-03&l=\n",
      "INFO: Querying Cairo since:2013-04-15 until:2013-12-29\n",
      "INFO: Querying Cairo since:2018-12-11 until:2019-08-26\n",
      "INFO: Querying Cairo since:2018-03-28 until:2018-12-11\n",
      "INFO: Querying Cairo since:2010-06-17 until:2011-03-02\n",
      "INFO: Querying Cairo since:2015-05-30 until:2016-02-12\n",
      "INFO: Querying Cairo since:2009-10-02 until:2010-06-17\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2011-11-16%20until%3A2012-07-31&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2008-05-03%20until%3A2009-01-17&l=\n",
      "INFO: Querying Cairo since:2017-07-13 until:2018-03-28\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2013-12-29%20until%3A2014-09-14&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2011-03-02%20until%3A2011-11-16&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2013-04-15%20until%3A2013-12-29&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2018-12-11%20until%3A2019-08-26&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2014-09-14%20until%3A2015-05-30&l=\n",
      "INFO: Querying Cairo since:2019-08-26 until:2020-05-11\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2010-06-17%20until%3A2011-03-02&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2009-01-17%20until%3A2009-10-02&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2009-10-02%20until%3A2010-06-17&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2016-10-27%20until%3A2017-07-13&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2016-02-12%20until%3A2016-10-27&l=\n",
      "INFO: Querying Cairo since:2012-07-31 until:2013-04-15\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2017-07-13%20until%3A2018-03-28&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2015-05-30%20until%3A2016-02-12&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2018-03-28%20until%3A2018-12-11&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2019-08-26%20until%3A2020-05-11&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2012-07-31%20until%3A2013-04-15&l=\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Using proxy 36.89.183.77:53557\n",
      "INFO: Retrying... (Attempts left: 50)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 35.220.131.188:80\n",
      "INFO: Got 16 tweets for Cairo%20since%3A2009-10-02%20until%3A2010-06-17.\n",
      "INFO: Got 16 tweets (16 new).\n",
      "INFO: Got 19 tweets for Cairo%20since%3A2010-06-17%20until%3A2011-03-02.\n",
      "INFO: Got 35 tweets (19 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2006-12-04%20until%3A2007-08-19.\n",
      "INFO: Got 55 tweets (20 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2014-09-14%20until%3A2015-05-30.\n",
      "INFO: Got 75 tweets (20 new).\n",
      "INFO: Got 15 tweets for Cairo%20since%3A2011-11-16%20until%3A2012-07-31.\n",
      "INFO: Got 90 tweets (15 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2008-05-03%20until%3A2009-01-17.\n",
      "INFO: Got 110 tweets (20 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2007-08-19%20until%3A2008-05-03.\n",
      "INFO: Got 130 tweets (20 new).\n",
      "INFO: Got 15 tweets for Cairo%20since%3A2016-10-27%20until%3A2017-07-13.\n",
      "INFO: Got 145 tweets (15 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2013-12-29%20until%3A2014-09-14.\n",
      "INFO: Got 165 tweets (20 new).\n",
      "INFO: Got 19 tweets for Cairo%20since%3A2016-02-12%20until%3A2016-10-27.\n",
      "INFO: Got 184 tweets (19 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2015-05-30%20until%3A2016-02-12.\n",
      "INFO: Got 204 tweets (20 new).\n",
      "INFO: Retrying... (Attempts left: 49)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 193.193.240.34:45944\n",
      "INFO: Got 19 tweets for Cairo%20since%3A2012-07-31%20until%3A2013-04-15.\n",
      "INFO: Got 223 tweets (19 new).\n",
      "INFO: Got 15 tweets for Cairo%20since%3A2019-08-26%20until%3A2020-05-11.\n",
      "INFO: Got 18 tweets for Cairo%20since%3A2009-01-17%20until%3A2009-10-02.\n",
      "INFO: Got 238 tweets (15 new).\n",
      "INFO: Got 256 tweets (18 new).\n",
      "INFO: Got 18 tweets for Cairo%20since%3A2011-03-02%20until%3A2011-11-16.\n",
      "INFO: Got 274 tweets (18 new).\n",
      "INFO: Got 17 tweets for Cairo%20since%3A2017-07-13%20until%3A2018-03-28.\n",
      "INFO: Got 291 tweets (17 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2018-12-11%20until%3A2019-08-26.\n",
      "INFO: Got 311 tweets (20 new).\n",
      "INFO: Got 18 tweets for Cairo%20since%3A2018-03-28%20until%3A2018-12-11.\n",
      "INFO: Got 329 tweets (18 new).\n",
      "INFO: Got 20 tweets for Cairo%20since%3A2013-04-15%20until%3A2013-12-29.\n",
      "INFO: Got 349 tweets (20 new).\n",
      "INFO: Retrying... (Attempts left: 48)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 186.46.222.226:59765\n",
      "INFO: Retrying... (Attempts left: 47)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.250.70.9:51370\n",
      "INFO: Retrying... (Attempts left: 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 47.245.28.60:8118\n",
      "INFO: Retrying... (Attempts left: 45)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 109.110.73.106:32479\n",
      "INFO: Retrying... (Attempts left: 44)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 187.111.176.62:8080\n",
      "INFO: Retrying... (Attempts left: 43)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 123.200.20.242:58847\n",
      "INFO: Retrying... (Attempts left: 42)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 79.175.57.77:55477\n",
      "INFO: Retrying... (Attempts left: 41)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 46.63.71.13:8080\n",
      "INFO: Retrying... (Attempts left: 40)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 137.59.161.163:32372\n",
      "INFO: Retrying... (Attempts left: 39)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 54.175.186.38:8118\n",
      "INFO: Retrying... (Attempts left: 38)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 185.216.231.184:8080\n",
      "INFO: Retrying... (Attempts left: 37)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 191.232.167.197:80\n",
      "INFO: Retrying... (Attempts left: 36)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 191.241.226.230:53281\n",
      "INFO: Retrying... (Attempts left: 35)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.75.161.153:21776\n",
      "INFO: Retrying... (Attempts left: 34)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 36.67.153.58:48133\n",
      "INFO: Retrying... (Attempts left: 33)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.239.147.129:60323\n",
      "INFO: Retrying... (Attempts left: 32)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 188.168.96.34:34298\n",
      "INFO: Retrying... (Attempts left: 31)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 220.247.174.238:40768\n",
      "INFO: Retrying... (Attempts left: 30)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 36.89.181.161:50204\n",
      "INFO: Retrying... (Attempts left: 29)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 191.36.244.230:51377\n",
      "INFO: Retrying... (Attempts left: 28)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.92.212.242:43399\n",
      "INFO: Retrying... (Attempts left: 27)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.82.42.33:51811\n",
      "INFO: Retrying... (Attempts left: 26)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 177.66.255.178:49625\n",
      "INFO: Retrying... (Attempts left: 25)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 186.42.186.202:56569\n",
      "INFO: Retrying... (Attempts left: 24)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 103.15.167.122:41787\n",
      "INFO: Retrying... (Attempts left: 23)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 114.199.80.99:8182\n",
      "INFO: Retrying... (Attempts left: 22)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 5.23.102.194:52803\n",
      "INFO: Retrying... (Attempts left: 21)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 118.174.220.61:60070\n",
      "INFO: Retrying... (Attempts left: 20)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 104.148.76.146:3128\n",
      "INFO: Retrying... (Attempts left: 19)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 223.204.182.109:8080\n",
      "INFO: Retrying... (Attempts left: 18)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 14.102.61.138:49569\n",
      "INFO: Retrying... (Attempts left: 17)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 178.130.94.207:8080\n",
      "INFO: Retrying... (Attempts left: 16)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 138.204.71.211:8080\n",
      "INFO: Retrying... (Attempts left: 15)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 14.207.206.73:8080\n",
      "INFO: Retrying... (Attempts left: 14)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 112.78.41.230:808\n",
      "INFO: Retrying... (Attempts left: 13)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 36.68.202.115:8080\n",
      "INFO: Retrying... (Attempts left: 12)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 213.6.28.93:8080\n",
      "INFO: Retrying... (Attempts left: 11)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 220.247.174.235:40768\n",
      "INFO: Retrying... (Attempts left: 10)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 109.92.210.250:32231\n",
      "INFO: Retrying... (Attempts left: 9)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 190.147.93.2:52925\n",
      "INFO: Retrying... (Attempts left: 8)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 109.121.207.165:48834\n",
      "INFO: Retrying... (Attempts left: 7)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using proxy 41.58.162.46:34794\n",
      "INFO: Retrying... (Attempts left: 6)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 46.145.177.161:40330\n",
      "INFO: Retrying... (Attempts left: 5)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 115.178.25.130:43111\n",
      "INFO: Retrying... (Attempts left: 4)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 79.135.240.254:60053\n",
      "INFO: Retrying... (Attempts left: 3)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 169.159.190.25:53281\n",
      "INFO: Retrying... (Attempts left: 2)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 197.211.238.220:54675\n",
      "INFO: Retrying... (Attempts left: 1)\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Cairo%20since%3A2006-03-21%20until%3A2006-12-04&l=\n",
      "INFO: Using proxy 202.62.84.210:53281\n",
      "INFO: Got 0 tweets for Cairo%20since%3A2006-03-21%20until%3A2006-12-04.\n",
      "INFO: Got 349 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @writeawayuk: Cairo Jim in Search for Martenarten\n",
      "Fairfax man returning from Yemen stranded in Cairo after landing on no-fly list: \"I don't know any other way to ge... http://bit.ly/b0mJNJ\n",
      "WOW Egypt to play two matches outside Cairo http://bit.ly/99EjxN\n",
      "[latest news] Fairfax man returning from Yemen stranded in Cairo after landing on no-fly list: A ... http://bit.ly/bALcAY [Washington Post]\n",
      "http://bit.ly/J8yzt Fairfax man returning from Yemen stranded in Cairo after landing on no-fly list: A Fairfax Cou... http://bit.ly/d5u8PL\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "for tweet in query_tweets(\"Cairo\", limit=10)[:5]:\n",
    "    print(str(getattr(tweet, 'text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to search for ? : Football\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Football since:2020-04-11 until:2020-04-12', 'Football since:2020-04-12 until:2020-04-14', 'Football since:2020-04-14 until:2020-04-15', 'Football since:2020-04-15 until:2020-04-17', 'Football since:2020-04-17 until:2020-04-18', 'Football since:2020-04-18 until:2020-04-20', 'Football since:2020-04-20 until:2020-04-21', 'Football since:2020-04-21 until:2020-04-23', 'Football since:2020-04-23 until:2020-04-24', 'Football since:2020-04-24 until:2020-04-26', 'Football since:2020-04-26 until:2020-04-27', 'Football since:2020-04-27 until:2020-04-29', 'Football since:2020-04-29 until:2020-04-30', 'Football since:2020-04-30 until:2020-05-02', 'Football since:2020-05-02 until:2020-05-03', 'Football since:2020-05-03 until:2020-05-05', 'Football since:2020-05-05 until:2020-05-06', 'Football since:2020-05-06 until:2020-05-08', 'Football since:2020-05-08 until:2020-05-09', 'Football since:2020-05-09 until:2020-05-11']\n",
      "INFO: Querying Football since:2020-04-21 until:2020-04-23\n",
      "INFO: Querying Football since:2020-04-12 until:2020-04-14\n",
      "INFO: Querying Football since:2020-04-11 until:2020-04-12\n",
      "INFO: Querying Football since:2020-04-15 until:2020-04-17\n",
      "INFO: Querying Football since:2020-04-14 until:2020-04-15\n",
      "INFO: Querying Football since:2020-04-20 until:2020-04-21\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-11%20until%3A2020-04-12&l=\n",
      "INFO: Querying Football since:2020-04-27 until:2020-04-29\n",
      "INFO: Querying Football since:2020-05-02 until:2020-05-03\n",
      "INFO: Querying Football since:2020-04-26 until:2020-04-27\n",
      "INFO: Querying Football since:2020-04-30 until:2020-05-02\n",
      "INFO: Querying Football since:2020-04-24 until:2020-04-26\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-14%20until%3A2020-04-15&l=\n",
      "INFO: Querying Football since:2020-04-29 until:2020-04-30\n",
      "INFO: Querying Football since:2020-04-18 until:2020-04-20\n",
      "INFO: Querying Football since:2020-04-23 until:2020-04-24\n",
      "INFO: Querying Football since:2020-04-17 until:2020-04-18\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-12%20until%3A2020-04-14&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-24%20until%3A2020-04-26&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-29%20until%3A2020-04-30&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-18%20until%3A2020-04-20&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-15%20until%3A2020-04-17&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-27%20until%3A2020-04-29&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-30%20until%3A2020-05-02&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-17%20until%3A2020-04-18&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-20%20until%3A2020-04-21&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-23%20until%3A2020-04-24&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-26%20until%3A2020-04-27&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Querying Football since:2020-05-08 until:2020-05-09\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-04-21%20until%3A2020-04-23&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Querying Football since:2020-05-03 until:2020-05-05\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Querying Football since:2020-05-09 until:2020-05-11\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-08%20until%3A2020-05-09&l=\n",
      "INFO: Querying Football since:2020-05-06 until:2020-05-08\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Querying Football since:2020-05-05 until:2020-05-06\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-03%20until%3A2020-05-05&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-02%20until%3A2020-05-03&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-09%20until%3A2020-05-11&l=\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-05%20until%3A2020-05-06&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=Football%20since%3A2020-05-06%20until%3A2020-05-08&l=\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Using proxy 41.190.95.20:56167\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-20%20until%3A2020-04-21.\n",
      "INFO: Got 20 tweets (20 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-30%20until%3A2020-05-02.\n",
      "INFO: Got 40 tweets (20 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-05-03%20until%3A2020-05-05.\n",
      "INFO: Got 58 tweets (18 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-27%20until%3A2020-04-29.\n",
      "INFO: Got 78 tweets (20 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-11%20until%3A2020-04-12.\n",
      "INFO: Got 98 tweets (20 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-17%20until%3A2020-04-18.\n",
      "INFO: Got 118 tweets (20 new).\n",
      "INFO: Got 17 tweets for Football%20since%3A2020-04-23%20until%3A2020-04-24.\n",
      "INFO: Got 135 tweets (17 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-14%20until%3A2020-04-15.\n",
      "INFO: Got 155 tweets (20 new).\n",
      "INFO: Got 19 tweets for Football%20since%3A2020-05-05%20until%3A2020-05-06.\n",
      "INFO: Got 174 tweets (19 new).\n",
      "INFO: Got 19 tweets for Football%20since%3A2020-04-24%20until%3A2020-04-26.\n",
      "INFO: Got 193 tweets (19 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-05-09%20until%3A2020-05-11.\n",
      "INFO: Got 211 tweets (18 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-18%20until%3A2020-04-20.\n",
      "INFO: Got 231 tweets (20 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-05-08%20until%3A2020-05-09.\n",
      "INFO: Got 249 tweets (18 new).\n",
      "INFO: Got 19 tweets for Football%20since%3A2020-05-02%20until%3A2020-05-03.\n",
      "INFO: Got 268 tweets (19 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-04-29%20until%3A2020-04-30.\n",
      "INFO: Got 288 tweets (20 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-04-21%20until%3A2020-04-23.\n",
      "INFO: Got 306 tweets (18 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-04-12%20until%3A2020-04-14.\n",
      "INFO: Got 324 tweets (18 new).\n",
      "INFO: Got 17 tweets for Football%20since%3A2020-04-26%20until%3A2020-04-27.\n",
      "INFO: Got 341 tweets (17 new).\n",
      "INFO: Got 18 tweets for Football%20since%3A2020-04-15%20until%3A2020-04-17.\n",
      "INFO: Got 359 tweets (18 new).\n",
      "INFO: Got 20 tweets for Football%20since%3A2020-05-06%20until%3A2020-05-08.\n",
      "INFO: Got 379 tweets (20 new).\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from twitterscraper import query_tweets\n",
    "\n",
    "query = input('What do you want to search for ? : ')\n",
    "\n",
    "begin = dt.date(2020,4,11)\n",
    "tweets = query_tweets(query,begindate=begin,limit=5)\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "text = df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      cristiano ronaldo le joueur de football de la ...\n",
       "1      so you d take a pay cut even though none of th...\n",
       "2                                    football season wya\n",
       "3      nothing say quarantine like vodka drink on a n...\n",
       "4                 this be awesome dawson congratulations\n",
       "                             ...                        \n",
       "374    for those of you who be mourn the think of hav...\n",
       "375    football schedule be out go to start plan a tr...\n",
       "376    i d rather not watch football than be a lions fan\n",
       "377    yes sir great job brotha man declaretobegreath...\n",
       "378    thanksgiving football on my loft tv will be amaze\n",
       "Name: text, Length: 379, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_predicted = clf.predict(transformed)\n",
    "plt.plot(y_predicted,'o')\n",
    "plt.show()\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try CountVectorizer instead of TFIDF Vectorizer and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english',max_features=100000,dtype=np.int16)\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "new_clf = MultinomialNB()\n",
    "new_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = new_clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "new_clf = SGDClassifier()\n",
    "\n",
    "new_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = new_clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "new_clf3 = LinearSVC(class_weight='balanced')\n",
    "new_clf3.fit(x_train,y_train)\n",
    "y_predicted = new_clf3.predict(x_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train)\n",
    "y_predicted = xgb.predict(x_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ENTER YOUR QUERY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start by building your sentiment analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now time to collect some tweets based on the given query\n",
    "\n",
    "for this task you can use multiple libraries, one of the very best ones is [twitterscraper by taspinar](https://github.com/taspinar/twitterscraper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now apply the model you have trained on the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some good looking charts \n",
    "\n",
    "use your favorite plotting library to plot the sentiment aggregation of the tweets you have gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good job ! \n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/YRuFixSNWFVcXaxpmX/giphy.gif\"></center>\n",
    "\n",
    "# Now publish the notebook and show the world your work !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
